package org;

import java.io.*;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.util.*;
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.TimeUnit;
import java.util.logging.Logger;
import java.util.regex.Matcher;
import java.util.regex.Pattern;
import java.util.stream.Collectors;

/**
 * ProsodyAnalysisUtils - An√°lise avan√ßada de pros√≥dia, emo√ß√µes e sil√™ncios
 * Para timing perfeito na dublagem com Piper TTS
 */
public class ProsodyAnalysisUtils {
    
    private static final Logger logger = Logger.getLogger(ProsodyAnalysisUtils.class.getName());
    
    // Configura√ß√µes de ferramentas externas
    private static final String PRAAT_EXECUTABLE = "/usr/bin/praat";
    private static final String OPENSMILE_EXECUTABLE = "/opt/opensmile/bin/SMILExtract";
    private static final String OPENSMILE_CONFIG = "/opt/opensmile/config/eGeMAPSv02.conf";
    
    // Thresholds para detec√ß√£o
    private static final double SILENCE_THRESHOLD_DB = -35.0;
    private static final double MIN_SILENCE_DURATION = 0.05; // 50ms
    private static final double INTER_WORD_THRESHOLD = 0.1;  // 100ms
    private static final double BREATH_THRESHOLD = 0.3;     // 300ms
    
    /**
     * An√°lise completa de pros√≥dia para um arquivo de √°udio
     */
    public static CompletableFuture<ProsodyAnalysisResult> analyzeComplete(Path audioFile) {
        return CompletableFuture.supplyAsync(() -> {
            try {
                logger.info("üéØ Iniciando an√°lise pros√≥dica completa: " + audioFile.getFileName());
                
                // An√°lises paralelas
                CompletableFuture<List<SilenceSegment>> silenceAnalysis = 
                    CompletableFuture.supplyAsync(() -> extractDetailedSilences(audioFile));
                
                CompletableFuture<EmotionMetrics> emotionAnalysis = 
                    CompletableFuture.supplyAsync(() -> analyzeEmotionalState(audioFile));
                
                CompletableFuture<ProsodyMetrics> prosodyAnalysis = 
                    CompletableFuture.supplyAsync(() -> analyzePitchIntensity(audioFile));
                
                // Combinar resultados
                return new ProsodyAnalysisResult(
                    audioFile,
                    silenceAnalysis.get(),
                    emotionAnalysis.get(),
                    prosodyAnalysis.get()
                );
                
            } catch (Exception e) {
                logger.warning("‚ö†Ô∏è Erro na an√°lise pros√≥dica: " + e.getMessage());
                return createFallbackAnalysis(audioFile);
            }
        });
    }
    
    /**
     * Extra√ß√£o detalhada de sil√™ncios com classifica√ß√£o
     */
    public static List<SilenceSegment> extractDetailedSilences(Path audioFile) {
        List<SilenceSegment> silences = new ArrayList<>();
        
        try {
            // FFmpeg com silencedetect mais sens√≠vel
            ProcessBuilder pb = new ProcessBuilder(
                "ffmpeg", "-i", audioFile.toString(),
                "-af", String.format("silencedetect=noise=%.1fdB:duration=%.3f", 
                       SILENCE_THRESHOLD_DB, MIN_SILENCE_DURATION),
                "-f", "null", "-", "-y"
            );
            
            pb.redirectErrorStream(true);
            Process process = pb.start();
            
            List<String> output = new ArrayList<>();
            try (BufferedReader reader = new BufferedReader(new InputStreamReader(process.getInputStream()))) {
                String line;
                while ((line = reader.readLine()) != null) {
                    output.add(line);
                }
            }
            
            boolean finished = process.waitFor(30, TimeUnit.SECONDS);
            if (!finished) {
                process.destroyForcibly();
                return Collections.emptyList();
            }
            
            // Parse das detec√ß√µes de sil√™ncio
            silences = parseSilenceOutput(output);
            
            logger.info(String.format("üîá Detectados %d sil√™ncios em %s", 
                silences.size(), audioFile.getFileName()));
            
        } catch (Exception e) {
            logger.warning("‚ö†Ô∏è Erro na detec√ß√£o de sil√™ncios: " + e.getMessage());
        }
        
        return silences;
    }
    
    private static List<SilenceSegment> parseSilenceOutput(List<String> output) {
        List<SilenceSegment> silences = new ArrayList<>();
        
        Pattern startPattern = Pattern.compile("silence_start: ([0-9.]+)");
        Pattern endPattern = Pattern.compile("silence_end: ([0-9.]+)");
        
        Double silenceStart = null;
        
        for (String line : output) {
            Matcher startMatcher = startPattern.matcher(line);
            Matcher endMatcher = endPattern.matcher(line);
            
            if (startMatcher.find()) {
                silenceStart = Double.parseDouble(startMatcher.group(1));
            } else if (endMatcher.find() && silenceStart != null) {
                double silenceEnd = Double.parseDouble(endMatcher.group(1));
                double duration = silenceEnd - silenceStart;
                
                SilenceType type = classifySilenceType(duration);
                silences.add(new SilenceSegment(silenceStart, silenceEnd, duration, type));
                
                silenceStart = null;
            }
        }
        
        return silences;
    }
    
    private static SilenceType classifySilenceType(double duration) {
        if (duration < INTER_WORD_THRESHOLD) {
            return SilenceType.INTER_WORD;
        } else if (duration < BREATH_THRESHOLD) {
            return SilenceType.PAUSE;
        } else if (duration < 1.0) {
            return SilenceType.BREATH;
        } else {
            return SilenceType.LONG_PAUSE;
        }
    }
    
    /**
     * An√°lise emocional usando OpenSMILE (se dispon√≠vel)
     */
    public static EmotionMetrics analyzeEmotionalState(Path audioFile) {
        try {
            // Verificar se OpenSMILE est√° dispon√≠vel
            if (!Files.exists(Paths.get(OPENSMILE_EXECUTABLE))) {
                logger.info("üìä OpenSMILE n√£o dispon√≠vel, usando an√°lise b√°sica");
                return analyzeEmotionBasic(audioFile);
            }
            
            Path csvOutput = Files.createTempFile("emotion_analysis", ".csv");
            
            ProcessBuilder pb = new ProcessBuilder(
                OPENSMILE_EXECUTABLE,
                "-C", OPENSMILE_CONFIG,
                "-I", audioFile.toString(),
                "-O", csvOutput.toString()
            );
            
            pb.redirectErrorStream(true);
            Process process = pb.start();
            
            boolean finished = process.waitFor(60, TimeUnit.SECONDS);
            if (!finished) {
                process.destroyForcibly();
                return createNeutralEmotion();
            }
            
            if (process.exitValue() == 0 && Files.exists(csvOutput)) {
                return parseEmotionFeatures(csvOutput);
            }
            
        } catch (Exception e) {
            logger.warning("‚ö†Ô∏è Erro na an√°lise emocional OpenSMILE: " + e.getMessage());
        }
        
        return analyzeEmotionBasic(audioFile);
    }
    
    private static EmotionMetrics analyzeEmotionBasic(Path audioFile) {
        try {
            // An√°lise b√°sica usando FFmpeg para extrair features simples
            ProcessBuilder pb = new ProcessBuilder(
                "ffmpeg", "-i", audioFile.toString(),
                "-af", "astats=metadata=1:reset=1",
                "-f", "null", "-", "-y"
            );
            
            pb.redirectErrorStream(true);
            Process process = pb.start();
            
            StringBuilder output = new StringBuilder();
            try (BufferedReader reader = new BufferedReader(new InputStreamReader(process.getInputStream()))) {
                String line;
                while ((line = reader.readLine()) != null) {
                    output.append(line).append("\n");
                }
            }
            
            process.waitFor(30, TimeUnit.SECONDS);
            
            // Parse b√°sico de caracter√≠sticas do √°udio
            return parseBasicAudioFeatures(output.toString());
            
        } catch (Exception e) {
            logger.warning("‚ö†Ô∏è Erro na an√°lise emocional b√°sica: " + e.getMessage());
            return createNeutralEmotion();
        }
    }
    
    private static EmotionMetrics parseEmotionFeatures(Path csvFile) {
        try {
            List<String> lines = Files.readAllLines(csvFile);
            if (lines.size() < 2) {
                return createNeutralEmotion();
            }
            
            // Parse do CSV OpenSMILE (formato eGeMAPS)
            String[] headers = lines.get(0).split(",");
            String[] values = lines.get(1).split(",");
            
            Map<String, Double> features = new HashMap<>();
            for (int i = 0; i < Math.min(headers.length, values.length); i++) {
                try {
                    features.put(headers[i].trim(), Double.parseDouble(values[i].trim()));
                } catch (NumberFormatException e) {
                    // Ignorar valores n√£o num√©ricos
                }
            }
            
            // Extrair m√©tricas relevantes
            double f0mean = features.getOrDefault("F0semitoneFrom27.5Hz_sma3nz_amean", 0.0);
            double intensity = features.getOrDefault("loudness_sma3_amean", 0.0);
            double spectralCentroid = features.getOrDefault("spectralCentroidUsingPower_sma3_amean", 0.0);
            
            // Estimar valence e arousal baseado nas features
            double valence = estimateValence(f0mean, intensity, spectralCentroid);
            double arousal = estimateArousal(intensity, spectralCentroid);
            
            return new EmotionMetrics(valence, arousal, 0.5, createEmotionMap(valence, arousal));
            
        } catch (Exception e) {
            logger.warning("‚ö†Ô∏è Erro parsing features emocionais: " + e.getMessage());
            return createNeutralEmotion();
        } finally {
            try {
                Files.deleteIfExists(csvFile);
            } catch (Exception e) {
                // Ignorar erro na limpeza
            }
        }
    }
    
    private static EmotionMetrics parseBasicAudioFeatures(String ffmpegOutput) {
        // An√°lise b√°sica baseada em estat√≠sticas do √°udio
        double rms = extractStatistic(ffmpegOutput, "RMS level");
        double peak = extractStatistic(ffmpegOutput, "Peak level");
        double dynamicRange = Math.abs(peak - rms);
        
        // Estimativas b√°sicas
        double arousal = Math.min(1.0, dynamicRange / 40.0); // Normalizar dynamic range
        double valence = 0.5; // Neutro por padr√£o
        
        return new EmotionMetrics(valence, arousal, 0.5, createEmotionMap(valence, arousal));
    }
    
    private static double extractStatistic(String output, String statName) {
        Pattern pattern = Pattern.compile(statName + ".*?([\\-0-9.]+)\\s*dB");
        Matcher matcher = pattern.matcher(output);
        if (matcher.find()) {
            try {
                return Double.parseDouble(matcher.group(1));
            } catch (NumberFormatException e) {
                return 0.0;
            }
        }
        return 0.0;
    }
    
    private static double estimateValence(double f0mean, double intensity, double spectralCentroid) {
        // Estimativa simples baseada em features ac√∫sticas
        double valence = 0.5; // Base neutra
        
        if (f0mean > 200) valence += 0.2; // Pitch alto = mais positivo
        if (intensity > -20) valence += 0.1; // Mais intenso = mais positivo  
        if (spectralCentroid > 2000) valence += 0.1; // Mais brilhante = mais positivo
        
        return Math.max(0.0, Math.min(1.0, valence));
    }
    
    private static double estimateArousal(double intensity, double spectralCentroid) {
        // Estimativa baseada em energia e brilho espectral
        double arousal = 0.3; // Base baixa
        
        if (intensity > -15) arousal += 0.4; // Muito intenso = alto arousal
        if (spectralCentroid > 2500) arousal += 0.3; // Muito brilhante = alto arousal
        
        return Math.max(0.0, Math.min(1.0, arousal));
    }
    
    private static Map<String, Double> createEmotionMap(double valence, double arousal) {
        Map<String, Double> emotions = new HashMap<>();
        
        // Mapear valence/arousal para emo√ß√µes b√°sicas
        if (valence > 0.6 && arousal > 0.6) {
            emotions.put("joy", 0.8);
            emotions.put("excitement", 0.7);
        } else if (valence < 0.4 && arousal > 0.6) {
            emotions.put("anger", 0.7);
            emotions.put("stress", 0.6);
        } else if (valence < 0.4 && arousal < 0.4) {
            emotions.put("sadness", 0.6);
            emotions.put("melancholy", 0.5);
        } else {
            emotions.put("neutral", 0.8);
            emotions.put("calm", 0.6);
        }
        
        return emotions;
    }
    
    /**
     * An√°lise de pitch e intensidade usando Praat (se dispon√≠vel)
     */
    public static ProsodyMetrics analyzePitchIntensity(Path audioFile) {
        try {
            if (!Files.exists(Paths.get(PRAAT_EXECUTABLE))) {
                logger.info("üéµ Praat n√£o dispon√≠vel, usando an√°lise FFmpeg");
                return analyzeProsodyWithFFmpeg(audioFile);
            }
            
            return analyzeProsodyWithPraat(audioFile);
            
        } catch (Exception e) {
            logger.warning("‚ö†Ô∏è Erro na an√°lise pros√≥dica: " + e.getMessage());
            return createBasicProsodyMetrics();
        }
    }
    
    private static ProsodyMetrics analyzeProsodyWithFFmpeg(Path audioFile) {
        try {
            // An√°lise b√°sica de pitch usando FFmpeg
            ProcessBuilder pb = new ProcessBuilder(
                "ffmpeg", "-i", audioFile.toString(),
                "-af", "aformat=s16:44100,apulsator=hz=0.125,aresample=100",
                "-f", "null", "-", "-y"
            );
            
            pb.redirectErrorStream(true);
            Process process = pb.start();
            
            process.waitFor(30, TimeUnit.SECONDS);
            
            // Retornar m√©tricas b√°sicas estimadas
            return new ProsodyMetrics(
                Collections.emptyList(), // pitchContour
                Collections.emptyList(), // intensityContour  
                150.0, // averagePitch estimado
                25.0,  // pitchVariance estimada
                Collections.emptyList() // stressPattern
            );
            
        } catch (Exception e) {
            logger.warning("‚ö†Ô∏è Erro na an√°lise FFmpeg: " + e.getMessage());
            return createBasicProsodyMetrics();
        }
    }
    
    private static ProsodyMetrics analyzeProsodyWithPraat(Path audioFile) {
        // Implementa√ß√£o completa do Praat seria aqui
        // Por agora, retorna m√©tricas b√°sicas
        return createBasicProsodyMetrics();
    }
    
    private static ProsodyMetrics createBasicProsodyMetrics() {
        return new ProsodyMetrics(
            Collections.emptyList(),
            Collections.emptyList(),
            150.0,
            20.0,
            Collections.emptyList()
        );
    }
    
    private static EmotionMetrics createNeutralEmotion() {
        Map<String, Double> neutral = new HashMap<>();
        neutral.put("neutral", 0.9);
        neutral.put("calm", 0.7);
        return new EmotionMetrics(0.5, 0.3, 0.5, neutral);
    }
    
    private static ProsodyAnalysisResult createFallbackAnalysis(Path audioFile) {
        return new ProsodyAnalysisResult(
            audioFile,
            Collections.emptyList(),
            createNeutralEmotion(),
            createBasicProsodyMetrics()
        );
    }
}